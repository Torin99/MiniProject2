{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbf00d2",
   "metadata": {},
   "source": [
    "# <div align=\"center\">CP322-A Mini-Project 2: Machine Learning</div>\n",
    "## <div align=\"center\">Group 6</div>\n",
    "### <div align=\"center\">due on 12-Nov-2023 at 11:30 PM</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52edb19",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18f51c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4831ebc8",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10493e",
   "metadata": {},
   "source": [
    "### Dataset 1 (20 Newsgroup): \n",
    "\n",
    "Use the default train subset (subset=‘train’, and remove=([‘headers’, ‘footers’, ‘quotes’]) in  sklearn.datasets) to train the models and report the final performance on the test subset.  note: you need to start with the text data and convert the text to feature vectors. Please refer to https://scikitlearn.org/stable/tutorial/text_analytics/working_with_text_data.html for a tutorial on the steps needed for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83c95110",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "    categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "X = twenty_train.data\n",
    "Y = twenty_train.target\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b09c268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "2257\n",
      "2257\n",
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "comp.graphics\n",
      "[1 1 3 3 3 3 3 2 2 2]\n",
      "comp.graphics\n",
      "comp.graphics\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.med\n",
      "sci.med\n",
      "sci.med\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target_names)\n",
    "print(len(twenty_train.data))\n",
    "print(len(twenty_train.filenames))\n",
    "# print(X[0])\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n",
    "\n",
    "print(twenty_train.target_names[twenty_train.target[0]])\n",
    "print(twenty_train.target[:10])\n",
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eace33c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 177)\t0.15075567228888181\n",
      "  (0, 230)\t0.07537783614444091\n",
      "  (0, 587)\t0.07537783614444091\n",
      "  (0, 2326)\t0.15075567228888181\n",
      "  (0, 3062)\t0.07537783614444091\n",
      "  (0, 3166)\t0.07537783614444091\n",
      "  (0, 4017)\t0.15075567228888181\n",
      "  (0, 4378)\t0.07537783614444091\n",
      "  (0, 4808)\t0.07537783614444091\n",
      "  (0, 5195)\t0.07537783614444091\n",
      "  (0, 5201)\t0.07537783614444091\n",
      "  (0, 5285)\t0.07537783614444091\n",
      "  (0, 8696)\t0.30151134457776363\n",
      "  (0, 9031)\t0.22613350843332272\n",
      "  (0, 9338)\t0.07537783614444091\n",
      "  (0, 9801)\t0.07537783614444091\n",
      "  (0, 9805)\t0.15075567228888181\n",
      "  (0, 9932)\t0.07537783614444091\n",
      "  (0, 12014)\t0.07537783614444091\n",
      "  (0, 12051)\t0.07537783614444091\n",
      "  (0, 12541)\t0.07537783614444091\n",
      "  (0, 12833)\t0.15075567228888181\n",
      "  (0, 14085)\t0.07537783614444091\n",
      "  (0, 14281)\t0.15075567228888181\n",
      "  (0, 14676)\t0.07537783614444091\n",
      "  :\t:\n",
      "  (2256, 24052)\t0.07216878364870323\n",
      "  (2256, 25560)\t0.07216878364870323\n",
      "  (2256, 26254)\t0.07216878364870323\n",
      "  (2256, 26998)\t0.07216878364870323\n",
      "  (2256, 27031)\t0.14433756729740646\n",
      "  (2256, 27042)\t0.07216878364870323\n",
      "  (2256, 28900)\t0.07216878364870323\n",
      "  (2256, 29224)\t0.07216878364870323\n",
      "  (2256, 29993)\t0.07216878364870323\n",
      "  (2256, 30325)\t0.21650635094610968\n",
      "  (2256, 30498)\t0.07216878364870323\n",
      "  (2256, 30776)\t0.21650635094610968\n",
      "  (2256, 31077)\t0.07216878364870323\n",
      "  (2256, 31342)\t0.07216878364870323\n",
      "  (2256, 31945)\t0.07216878364870323\n",
      "  (2256, 32142)\t0.43301270189221935\n",
      "  (2256, 32233)\t0.07216878364870323\n",
      "  (2256, 32270)\t0.07216878364870323\n",
      "  (2256, 33078)\t0.07216878364870323\n",
      "  (2256, 33844)\t0.07216878364870323\n",
      "  (2256, 34923)\t0.07216878364870323\n",
      "  (2256, 35157)\t0.07216878364870323\n",
      "  (2256, 35350)\t0.07216878364870323\n",
      "  (2256, 35638)\t0.14433756729740646\n",
      "  (2256, 35690)\t0.07216878364870323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e00a616b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab51515f",
   "metadata": {},
   "source": [
    "### Dataset 2 (IMDB Reviews):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6926886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin reading files.\n",
      "name 'os' is not defined\n",
      "Error found in file: \n",
      "Done reading positive values.\n",
      "File reading complete.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_imdb():\n",
    "    print(\"Begin reading files.\")\n",
    "    directory = 'data/train/neg'\n",
    "    data = []\n",
    "    labels = []\n",
    "    file = ''\n",
    "    try:\n",
    "        for filename in os.listdir(directory):\n",
    "            f = open(os.path.join(directory, filename), encoding=\"utf-8\")\n",
    "            file = filename, \"neg\"\n",
    "            for i in f:\n",
    "                test = True\n",
    "                while test:\n",
    "                    test = False\n",
    "                    k = i.find('<br /><br />')\n",
    "                    if k != -1:\n",
    "                        i = i[:k] + ' ' + i[k+12:]\n",
    "                        test = True\n",
    "                    \n",
    "                data.append(i)\n",
    "                labels.append(0)\n",
    "                \n",
    "        print(\"Done reading negative values.\")\n",
    "        \n",
    "        directory = 'data/train/pos'\n",
    "        \n",
    "        for filename in os.listdir(directory):\n",
    "            print(filename)\n",
    "            f = open(os.path.join(directory, filename), encoding=\"utf-8\")\n",
    "            file = filename, \"pos\"\n",
    "            for i in f:\n",
    "             \n",
    "                test = True\n",
    "                while test:\n",
    "                    test = False\n",
    "                    k = i.find('<br /><br />')\n",
    "                    if k != -1:\n",
    "                        i = i[:k] + ' ' + i[k+12:]\n",
    "                        test = True\n",
    "                     \n",
    "                data.append(i)\n",
    "                labels.append(1)\n",
    "                \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(\"Error found in file:\", file)\n",
    "        \n",
    "    print(\"Done reading positive values.\")\n",
    "    npdata = np.array(data)\n",
    "    nplabels = np.array(labels)\n",
    "    \n",
    "    return npdata, nplabels\n",
    "  \n",
    "data, labels = read_imdb()\n",
    "print(\"File reading complete.\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b18c4fa",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5109455e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f38e107a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "11314\n",
      "11314\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# Fetch the 20 newsgroups dataset\n",
    "# newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Partiall dataset\n",
    "# categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "\n",
    "# Fetch the \"train\" subset of the data to be used\n",
    "# newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Vectorizing the text from the given dataset using TF-IDF to get the matrix\n",
    "data_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_tfidf = data_vectorizer.fit_transform(newsgroups[\"data\"])\n",
    "\n",
    "# Defining the k-fold cross-validation with 5 folds measures to obtain the 5 groups\n",
    "kFold = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)\n",
    "\n",
    "# Defining logistic regression model with 1000 interations\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "print(newsgroups[\"target_names\"])\n",
    "print(len(newsgroups[\"target\"]))\n",
    "print(len(newsgroups[\"data\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f503d818",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0190c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly classified data points: 403\n",
      "Incorrectly classified data points: 7129\n",
      "[ 7  5  0 ...  9  6 15]\n",
      "[ 2 12  7 ... 12  9  7]\n",
      "Mean accuracy score of decision tree: 0.05350504514073287\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "#borrow newsgroups from above cell\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset=\"test\", remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "test_tfidf = data_vectorizer.fit_transform(newsgroups_test[\"data\"])\n",
    "\n",
    "decision_tree_classifier = tree.DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\") #other params that can be changed but defaults should be okay otherwise\n",
    "\n",
    "decision_tree_classifier.fit(X_tfidf, newsgroups[\"target\"]) #train\n",
    "\n",
    "#test\n",
    "tree_results = decision_tree_classifier.predict(test_tfidf)\n",
    "\n",
    "correct_classification_count = 0\n",
    "incorrect_classification_count = 0\n",
    "correct_classifications = newsgroups_test[\"target\"]\n",
    "\n",
    "for i in range( len(tree_results) ):\n",
    "\n",
    "    if  tree_results[i] == correct_classifications[i]:\n",
    "        correct_classification_count += 1\n",
    "\n",
    "    else:\n",
    "        incorrect_classification_count += 1\n",
    "\n",
    "print(f\"Correctly classified data points: {correct_classification_count}\")\n",
    "print(f\"Incorrectly classified data points: {incorrect_classification_count}\")\n",
    "\n",
    "print(correct_classifications)\n",
    "print(tree_results)\n",
    "\n",
    "print(f\"Mean accuracy score of decision tree: {decision_tree_classifier.score(test_tfidf, correct_classifications)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a8ad7",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba31ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "svm_model = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b679695c",
   "metadata": {},
   "source": [
    "### Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4911e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5eeb7795",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498d99f",
   "metadata": {},
   "source": [
    "##### Dataset 1 (20 Newsgroup):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5234eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8606\n",
      "Precision: 0.8803\n",
      "Recall: 0.8606\n",
      "F1 Score: 0.8618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "rf_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "rf_precision = metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "rf_recall = metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "rf_f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Precision: {rf_precision:.4f}\")\n",
    "print(f\"Recall: {rf_recall:.4f}\")\n",
    "print(f\"F1 Score: {rf_f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faf0ef3",
   "metadata": {},
   "source": [
    "##### Dataset 2 (IMDB Reviews):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c72f9",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7283a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\twbm\\AppData\\Local\\Temp\\ipykernel_11924\\1104062206.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({\n",
      "C:\\Users\\twbm\\AppData\\Local\\Temp\\ipykernel_11924\\1104062206.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({\n",
      "C:\\Users\\twbm\\AppData\\Local\\Temp\\ipykernel_11924\\1104062206.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({\n",
      "C:\\Users\\twbm\\AppData\\Local\\Temp\\ipykernel_11924\\1104062206.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0        Random Forest  0.880811   0.895755  0.880811  0.881001\n",
      "1             AdaBoost  0.739916   0.755450  0.739916  0.740215\n",
      "2                  SVM  0.963663   0.964693  0.963663  0.963687\n",
      "3        Decision Tree  0.780245   0.781392  0.780245  0.780037\n",
      "4  Logistic Regression  0.945496   0.948900  0.945496  0.945737\n",
      "\n",
      "Best Models:\n",
      "Accuracy: SVM\n",
      "Precision: SVM\n",
      "Recall: SVM\n",
      "F1 Score: SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\twbm\\AppData\\Local\\Temp\\ipykernel_11924\\1104062206.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import pandas as pd\n",
    "\n",
    "# Define models with their hyperparameters\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=50, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Initialize KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a DataFrame to store performance metrics\n",
    "results_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# Model validation pipeline\n",
    "for model_name, model in models.items():\n",
    "    accuracy_scores = cross_val_score(model, X_tfidf, Y, cv=kf, scoring='accuracy')\n",
    "    precision_scores = cross_val_score(model, X_tfidf, Y, cv=kf, scoring='precision_weighted')\n",
    "    recall_scores = cross_val_score(model, X_tfidf, Y, cv=kf, scoring='recall_weighted')\n",
    "    f1_scores = cross_val_score(model, X_tfidf, Y, cv=kf, scoring='f1_weighted')\n",
    "\n",
    "    results_df = results_df.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_scores.mean(),\n",
    "        'Precision': precision_scores.mean(),\n",
    "        'Recall': recall_scores.mean(),\n",
    "        'F1 Score': f1_scores.mean()\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Print the results table\n",
    "print(results_df)\n",
    "\n",
    "# Identify the best model for each metric\n",
    "best_accuracy_model = results_df.loc[results_df['Accuracy'].idxmax()]['Model']\n",
    "best_precision_model = results_df.loc[results_df['Precision'].idxmax()]['Model']\n",
    "best_recall_model = results_df.loc[results_df['Recall'].idxmax()]['Model']\n",
    "best_f1_model = results_df.loc[results_df['F1 Score'].idxmax()]['Model']\n",
    "\n",
    "print(\"\\nBest Models:\")\n",
    "print(f\"Accuracy: {best_accuracy_model}\")\n",
    "print(f\"Precision: {best_precision_model}\")\n",
    "print(f\"Recall: {best_recall_model}\")\n",
    "print(f\"F1 Score: {best_f1_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7498ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
